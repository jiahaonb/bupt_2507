# 基于深度学习的点云目标检测算法实现

> 本任务旨在实现一个基于神经网络的三维点云目标检测系统，要求能够对原始或预处理后的三维点云数据进行目标识别与检测，输出目标的类别及其三维包围框信息。

任务目标：

1. 数据准备与预处理

   - 1.1 熟悉并解析点云数据格式（如 `.bin`、`.pcd`、`.ply` 等）；
   - 1.2 实现点云数据的归一化、裁剪、投影、采样等预处理操作；
   - 1.3 数据集划分合理，标注信息可用于训练监督。
2. 模型设计与训练：

    - 2.1 构建或改进点云目标检测网络；
    - 2.2 实现数据加载模块、网络前向传播、损失函数与训练过程
    - 2.3 模型需能输出目标类别与其三维边界框（center + size + orientation）。

3. 模型评估与测试：

    - 在测试集上进行模型性能评估；
    - 可视化检测结果（支持显示检测框与真实框对比）；
    - 评估指标包括但不限于：3D mAP（Mean Average Precision）、IoU、Precision、Recall 等。

4. 结果可视化与模型管理：

    - 支持训练过程中的损失曲线、mAP 曲线可视化；
    - 支持保存和加载训练权重；
    - 支持调用模型进行测试推理并输出结果。

对此，我们选择了数据集NuScenes, 并且采取开源的代码项目:`IS-Fusion`，来完成这个目标。由于数据集过大（300GB+），我们最终选择的是`NuScenes-mini`数据集(3GB+).

## 1. 查看方法

我们得知，`IS-Fusion`是CVPR'2024 论文，论文地址是`https://openaccess.thecvf.com/content/CVPR2024/papers/Yin_IS-Fusion_Instance-Scene_Collaborative_Fusion_for_Multimodal_3D_Object_Detection_CVPR_2024_paper.pdf`。该工作同时对实例级和场景级多模态上下文进行建模，以增强 3D 检测性能。我们可以查看对应的论文，了解其原理、方法和贡献，并且针对性地分析其问题所在。下面是我们读完论文的理解和感受。

### 1.1 3D目标检测的挑战

3D目标检测是自动驾驶、机器人和增强现实等领域的关键技术，其核心在于精确感知和理解三维空间。激光雷达（LiDAR）传感器通过发射激光束并测量其反射时间，能够生成环境的精确三维点云数据，为3D感知提供了关键的几何信息 。然而，原始点云数据本身带来了独特的、根本性的挑战，使得传统的计算机视觉技术（如在图像上大获成功的卷积神经网络，CNN）难以直接应用。

这些挑战主要源于点云的特征：

- 无序性：点云是点的集合，其数据记录的顺序与物体的几何形状无关。因此，处理点云的模型必须具备排列不变性，能够从无序的点集中学习到一致的特征 。
- 稀疏性：LiDAR采集的点云在三维空间中分布极不均匀且高度稀疏。绝大部分三维空间是空的，没有任何数据点 。若将整个空间划分为规则的网格（体素），超过90%的体素将是空的 。直接在这样的稀疏数据上应用密集卷积操作，会造成巨大的计算和内存资源浪费 。
- 不规则性：点云的密度随距离和物体表面材质的变化而变化。近处的物体点云密集，远处的物体则非常稀疏。这种不规则的采样密度给学习统一且鲁棒的特征表示带来了巨大困难 。

我们经过调研发现，对于以上的挑战，早期的解决方案大多依赖于人工设计的特征提取方法，例如将点云投影到鸟瞰图（Bird's-Eye View, BEV）或前视图，然后应用2D图像处理技术，或者将点云栅格化为三维体素并编码手工设计的特征 。但是投影或手工特征提取过程不可避免地会丢失原始点云中丰富的几何信息，限制了模型的最终性能 。所以我们开始探索能够直接从原始点云中学习特征的端到端方法，从而催生了两大主流技术路线：基于点的方法和基于体素的方法。

### 1.2 基于点的方法

这个方法是从2017年就有了比较前沿的突破：PointNet。这个的核心思想是为每个点独立地学习一个空间编码，然后通过一个对称函数（具体为最大池化操作）将所有点的特征聚合为一个全局特征向量，从而实现对输入点云排列的置换不变性 。

也就是说，pointnet对点云的无序性的处理，是十分巧妙的。此外，它还摒弃了任何形式的体素化或投影，直接从原始点云中提取特征，标志着3D深度学习从依赖手工特征向端到端学习的重大转变。

![1753803387687](image/report/1753803387687.png)

但是我们也知道，PointNet的一个根本性局限在于它孤立地处理每个点，并通过全局池化丢失了局部结构信息。这就意味着，它无法捕捉局部几何结构，在精细模式识别以及复杂场景中的表现不佳。

所以，为了克服这个难题，我们看到了其迭代：`Pointnet++`:它的核心贡献在于引入了一个分层的神经网络架构，能够以类似于CNN的方式，在不断增大的感受野中学习局部特征。所以我们就可以通过递归地在输入点集的嵌套分区上应用PointNet来捕捉局部到全局的结构信息 。这里的每一个层级，包括之前的pointnet层，还有额外的采样层以及分组层，方便我们找到局部的特征，然后最后聚合为整个物体或场景的高阶语义特征。

![1753803685376](image/report/1753803685376.png)

除此之外，我们的pointnet++还引入了两种密度自适应的策略：多尺度分组和多分辨率分组。多尺度分组对局部区域进行分组、特征提取，并且拼接不同尺度的特征信息。而多分辨率分组啧是结合了上一层抽象后的特征和当前层原始点的特征，以一种更高效的方式融合多分辨率信息 。这两个策略，十分有效地提升了处理点云中的，密度不均的问题。

### 1.3 基于体素的方法

与直接处理无序点集的方法不通，我们有一些方法是将不规则的点云转换为规则的三维网格（即体素），从而能够利用成熟且强大的3D卷积神经网络进行特征提取。这个就和我们的2d的目标检测方法就可以连上了。

例如，`VoxelNet`，它将点云转换为三维体素，然后通过3D卷积网络提取体素特征，最后通过RPN生成候选框。其包括`体素划分`，`点分组与采样`，`特征增强与编码`,`局部聚合`,`特征拼接与输出`几个部分，然后进行`FCN和最大池化操作`，并且最终通过RPN和NMS处理，得到了检测结果。完全就和我们的2d的目标检测方法是一样的流程。可惜的是，这个的效率并不是很高。不过这给我们带来了十分多的思路和思考。

![1753804336354](image/report/1753804336354.png)

所以，我们后面为了去节省计算的资源，提升计算的效率，大多数人就去做了稀疏性相关的改进。因为这个是非常可以节省我们的计算资源的。研究者们得到了一个`SECOND`的方法，主要是通过“稀疏卷积”的思想，只在非空体素上进行卷积计算（通过一种高效的数据结构和算法来管理非空体素的位置，并在卷积过程中只对这些活动位点及其邻域内的其他活动位点进行特征的收集和计算，完全跳过了对海量空体素的无效操作 ）。

![1753805012255](image/report/1753805012255.png)

幸运的是，这个方法，使得我们的速度大幅提升，也让我们了解到了，基于体素的方法有那么大的潜力，使得之后的复杂的3D网络成为可能。

### 1.4 检测方法进一步发展

由于基于点的方法和基于体素的方法 都有了很大的突破，我们就得到了一些综合的方法， 或者引入了其他方式的方法。

例如，pointRCNN，这个就是一个两阶段的检测方法：（1）第一阶段以自下而上的方式直接处理原始点云，通过点云分割将场景分为前景和背景，从而生成少量高质量的3D候选框。

![1753805090953](image/report/1753805090953.png)

（2）系统会将每个候选框内的点转换到标准坐标系（canonical coordinates）中以学习局部空间特征。最后，结合第一阶段学习到的全局语义特征，对候选框的位置、大小和类别进行精确的优化和预测，得出最终的检测结果。这个和我们之前的认知都是一样的。

随后，我们的3D目标检测，进入了多模态融合的路。

### 1.5 多模态融合/BEV

随着基于LiDAR的3D目标检测技术日趋成熟，我们可以有这个意识：单一的传感器，不会让我们的方法更加鲁棒性。只有融合了多模态的信息，我们才可以得到全面的、稳健的对世界的感知和认识。在此过程中，鸟瞰图（BEV）作为一种理想的中间表示，逐渐成为多模态融合的“通用语言”。

摄像头和LiDAR这两种自动驾驶中最核心的传感器，一个提供精准的几何特征，一个提供丰富的语义信息。这两个一结合，我们就可以构建出来对世界的一个比较精准的感知了。大家都知道这样做是好的，目前的挑战和问题就是，如何有效地对齐和融合这两种来源和表示空间完全不同的异构数据。

根据融合发生在网络架构中的阶段，大致可以分为早期融合、晚期融合和深度/中间融合。

- 早期融合：数据级融合，输入层面对原始数据或低级特征进行融合（如点云上色），能够让网络从一开始就学习跨模态的联合表示。但是问题是，对传感器之间的标定精度极为敏感，任何微小的标定误差都可能导致特征错位，引入噪声。同时，过早的融合，会容易丢失一个模态的特征，系统性能会急剧下降。

- 晚期融合：决策级融合，它让每个模态的数据通过独立的网络进行处理，直到最后生成各自的检测结果（如边界框），然后再对这些高层级的输出进行融合 。他的特点就是，架构模块化，易于实现和扩展。由于各分支独立，当某一传感器失效时，其他分支仍能正常工作，系统鲁棒性高。也就相当于同时进行了两个预测工作。但是缺点就是，完全放弃了在特征层面进行跨模态交互的机会，无法学习到模态间深层次、细粒度的关联信息，因此性能上限通常不高 。

- 深度/中间融合：目前性能最优的特征融合，使用各自独立的骨干网络从不同模态中提取特征，然后在网络的中间某个特征层面对这些特征图进行融合 。这种方法兼顾了两种极端策略的优点，既允许网络学习各模态的独立高级表示，又能在特征空间中进行丰富的跨模态信息交互。

我们来看例子。这里的BEVFusion，就是一种深度/中间融合的方法。这是一个里程碑的工作。主要因为BEV的以下特点：

（1）BEV提供了一个与自车相关的、俯瞰视角的2D网格表示。来自不同位置、不同朝向的多个摄像头以及LiDAR的特征，都可以通过坐标变换被投影到这个统一的BEV空间中，从而解决了多传感器视角对齐的难题 ；（2）BEV能够很好地同时保留LiDAR的几何结构信息（如物体的形状和相对位置）和来自图像的语义信息（通过视角转换赋予BEV网格语义特征）；（3）在摄像头的透视视图中，物体的大小会随着距离变化而变化（近大远小），且容易发生遮挡。而鸟瞰图不会，体在BEV空间中的尺寸和空间关系是固定的，这极大地简化了检测任务 ；（4）BEV是进行路径规划和运动控制等下游任务的自然表示，使得感知与决策规划模块可以无缝衔接。

由此，我们开始分析BEVFusion的架构。这也是我们的IS-Fusion的基准和对标。

![1753806038351](image/report/1753806038351.png)

![1753807081064](image/report/1753807081064.png)

上面是两个不同机构提出的BEVFusion的架构。但是他们核心思想都是一样的：将摄像头的图像特征和激光雷达的点云特征转换到同一个上帝视角的二维网格（BEV）中，是进行有效融合的最佳方式。而且流程都是：

1. 分别处理摄像头和激光雷达的输入，提取各自的特征，进行独立的编码;
2. 将多视角的摄像头图像特征通过视图转换模块（如LSS）投影到BEV空间，形成Camera BEV特征图。同时，也将点云处理成LiDAR BEV特征图。
3. 在BEV空间下，对来自不同传感器的特征图进行融合。
4. 将融合后的BEV特征输入到下游任务（如3D物体检测、路面分割）的解码器中，输出最终结果。

二者的区别也主要是在于多任务平台，另一个注重鲁棒性。

所以我们可以看到，BEVFusion是一个清晰的、大众的、通用的架构。主要的结构就两个：

（1） 独立的模态流，通过这个模态流，生成摄像头和LiDAR的BEV特征图；
（2） 共享BEV空间融合：简单的拼接，之后通过一个卷积层来整合信息并降低通道数。

上面就是我们的BEVFusion的核心思想。也是我们今天说的IS-Fusion的启蒙思想。

我们发现，尽管BEVFusion已经很完备了，但是其挑战就凸显了出来：融合，是场景不可知的，也就是网络必须在融合后的大量背景信息噪声中，自行学习去发现微弱的物体信号。所以我们今天研究的IS-Fusion就产生了，主要就是在思考，如果我们先融合整个场景，然后利用场景中的物体实例本身，去引导一次更具针对性的二次融合，效果会不会更好？

### 1.6 IS-Fusion

与上面的BEVFusion不同，IS-Fusion的核心论点是：现有主流的多模态融合方法，特别是那些完全依赖于在BEV空间进行场景级特征融合的模型，在融合过程中忽略了前景实例（如车辆、行人）与背景区域（如道路、建筑）之间的内在差异，这会稀释掉小物体的特征信号，不利于我们各种任务的鲁棒性。

故此，IS-Fusion的目标是设计一个能够显式地引入实例级多模态信息，并促进实例级特征与场景级特征之间协同工作的全新框架 。其最终目的是生成一个“实例感知”（instance-aware）的BEV表示，使得每个位置的特征都能够根据场景中的关键物体进行动态调整和增强。

![1753807893261](image/report/1753807893261.png)

我们来仔细分析其结构。我们可以看到，它的整体架构遵循一个清晰的流水线：

- 首先，通过模态特定的编码器提取初始特征；
- 然后，这些特征被送入其核心的两个模块：分层场景融合（Hierarchical Scene Fusion）和实例引导融合（Instance-Guided Fusion），最后由一个检测头输出最终结果。

所以我们就知晓了，其中最重要的就是这两个模块：

#### 1.6.1 分层场景融合 HSF

HSF旨在从不同粒度（点、网格、区域）上捕捉和融合多模态的场景上下文信息，从而生成一个信息丰富的初始BEV特征图 。他是通过一个两阶段的Transformer来完成这一任务的。

1. 点到网络Transformer：将点级别的特征聚合到BEV网格级别。首先就是将BEV柱内的LiDAR点投影到图像特征图上，以获取对应的像素级语义特征。这就完成了一次融合。为了应对LiDAR和相机之间可能存在的标定噪声，它在每个柱内部的所有点之间进行注意力计算，允许每个点“看到”一个更大的感受野，从而隐式地校正噪声点的影响。通过最大池化操作整合柱内的多模态信息，生成网格级别的BEV特征。

![1753808347396](image/report/1753808347396.png)

2. 网格到区域Transformer：旨在捕捉全局场景上下文。我们得到了网格特征之后，在所有网格特征上应用全局自注意力会带来巨大的计算开销。所以我们就将BEV网格分组为不同的区域，在每个区域内部通过注意力机制交换网格间的信息，并且通过移位操作，并在移位后的区域上再次进行自注意力计算。

通过上面两个transformer的方式，我们就实现了将信息从单个点传播到整个BEV区域，整合了局部和全局的多模态场景上下文。

#### 1.6.2 实例引导融合 IGF

IGF模块是IS-Fusion架构中最核心的创新，它实现了从“场景融合”到“实例引导场景融合”的范式转变。基本思想是挖掘每个物体实例周围的多模态上下文，并将这些关键的实例级信息整合回场景特征中，从而生成一个“实例感知”的BEV表示 。主要有三个步骤：

![1753808722442](image/report/1753808722442.png)

1. 实例候选选择：目标是从场景中高效地找出潜在的物体，它在HSF模块输出的场景特征图上应用一个关键点检测头，来预测每个网格的“中心度”（centerness），即该点成为物体中心的概率。在推理时，选取中心度得分最高的Top-K个点作为实例候选，并通过一个线性层将它们嵌入为实例查询

2. 实例上下文聚合：旨在丰富每个实例查询的特征表示。它包含两个方面的交互：首先，通过在所有实例查询之间进行自注意力（self-attention），来探索实例与实例之间的关系；其次，为了挖掘每个实例的局部上下文，它使用可变形注意力让每个实例查询去高效地从多模态特征图中采样并聚合其周围邻域的特征。使得获得的上下文比较精确稳健。

3. 实例到场景Transformer：这是实现协同融合的决定性步骤。它采用了一个交叉注意力机制，其中，全局BEV特征图中的每个网格特征充当查询q,而经过上一步骤丰富后的实例级特征则充当键k和值v，这个设计允许场景中的每一个位置都能主动地从场景中所有相关的、重要的物体实例中“拉取”信息。

#### 1.6.3 HSF和IGF融合

HSF和IGF两个模块之间的协同工作和创新的信息流，是我们的巧妙所在。

- HSF模块首先从底层传感器数据出发，以一种自下而上的方式（点 -> 网格 -> 区域），构建出一个全面但通用的（generic）全场景BEV表示。

- IGF模块则接收这个场景图，并以一种自顶向下的方式，首先识别出场景中的关键“角色”（即物体实例）。然后，它通过上下文聚合来丰富这些“角色”的特征。

- 最关键的一步是IGF将信息反馈给场景表示。经过丰富后的实例特征并不仅仅用于一个独立的预测分支，而是被用来引导和精炼由HSF生成的初始场景图。这最终产生了一个实例感知的BEV表示，其中背景区域的特征已经被其周围的前景物体所“上下文关联”和增强。

这种架构设计实现了从被动集成到主动引导的范式转变。BEVFusion等模型采用的是被动融合，如同将两张透明的地图（一张几何图，一张语义图）叠在一起，网络必须自己从重叠的像素中去分辨哪些信息是重要的。而IS-Fusion的IGF模块则像一个智能向导，先在地图上标出所有的预测点，然后让地图上的每一个位置都能参考这些点来更新自己的信息。

所以，我们最后得到，这一机制大大优化了前景-背景模糊性的问题。在最终的BEV图中，每个实例所在位置的特征表示由其自身丰富的特征所主导，而不是背景，从而为后续的检测头提供了更强、更清晰的信号。这表明，该领域正在从基于密集网格的、穷举式的预测，转向更稀疏、更高效的、将计算资源集中在场景最相关部分的查询机制。

我们在不同的论文里面找到了对比数据，并总结得到模型之间的对比：

![1753814858239](image/report/1753814858239.png)

![1753814878666](image/report/1753814878666.png)

我们可以看到，IS-Fusion的功能在目前是非常强的。

下面我们来复现`IS-Fusion`，并且分析其代码部分。

## 2. 流程展示

我们从这里开始，进行`IS-Fusion`的实验配置部分。包括环境配置、数据集准备、模型训练、模型评估、结果可视化等。我们会分析方法的实验过程，运行过程，从而建立起对方法的优缺点的评价。之后，就可以针对其中的问题提出必要的改进。

### 2.1 环境配置

我们的`IS-Fusion`是基于`PyTorch`的大框架的，基于 torch 1.10.1、mmdet 2.14.0、mmcv 1.4.0 和 mmdet3d 0.16.0。所以第一步是创建环境并且安装对应的包文件。

```bash
conda create -n isfusion python=3.7
conda activate isfusion
conda install pytorch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 -c pytorch
```

由于MMDetection3D 依赖于 MMDetection，因此 mmcv-full 是必需的。mmcv有其自己的安装工具:mim。我们可以通过这个安装。mim 是 OpenMMLab 项目的包管理工具。

```bash
pip install -U openmim
mim install mmcv
```

这里mim会直接选择最合适我们的mmcv-full版本。除此之外，我们也可以按照对应的cuda版本以及pytorch版本来选择其他的mmcv-full版本。

接下来就是mmdetection和MMSegmentation，也是我们的基础。

```bash
pip install mmdet==2.14.0
pip install mmsegmentation==0.14.1
```

下面是最重要的一部分，也就是安装mmdetection3d.这个使我们可以对点云等数据进行处理。这个是在Github的发布，所以我们需要下载源代码并且通过setup来进行安装。

```bash
git clone https://github.com/open-mmlab/mmdetection3d.git
cd mmdetection3d
pip install -v -e .
```

最后需要安装torchEX,这个是在`mmdet3d/ops/TorchEx`目录下。

```bash
cd mmdet3d/ops/TorchEx
pip install -v -e .
```

完成以上的安装，我们再使用pip安装一些基础的库，在`ISFusion/requirements.txt`中。一般来说，完成上面的步骤，就可以全部安装完这里面的包了。

```bash
pip install -r requirements.txt
```

最后，我们可以通过pip freeze > new_requirements.txt来保存我们的包，或者通过`conda env export > environment.yaml`来保存我们的环境文件，便于之后进行复现。其实原仓库里面， 已经有了docker的配置文件，我们可以直接在另一台机器中使用docker复现这个场景。

### 2.2 数据集准备

完成了环境的配置，就可以开始进行数据集的配置了。我们按照官网的配置，链接了我们的数据集：由于我们准备的是nuscenes-mini数据集，但是要求要放在`mmdetection3d/data/nuscenes`目录下。所以我们创建软链接：

```bash
ln -s /home/lishengjie/data/nuscenes-mini /home/lishengjie/study/jiahao/bupt_2507/ISFusion/data/nuscenes
```

然后对数据集进行配置：

```bash
python tools/create_data.py nuscenes --root-path ./data/nuscenes --out-dir ./data/nuscenes --extra-tag nuscenes --version v1.0-mini
```

我们可以观察到，这里的

### 2.3 模型训练

我们可以看到，在`tools/train.py`中，有对应的训练代码。然后我们可以调用`tools/run-nus.sh`，来针对性地对这个nuscenes数据集进行处理，并且开始训练的过程。\

```bash
bash tools/run-nus.sh 'tag'
```

后面设置‘tag’，方便我们查看log以及训练数据等。

我们可以看到，这个sh文件主要是加载一些参数，然后调用`tools/train.py`来开始训练。

![1753798518777](image/report/1753798518777.png)

里面我们设置了python第三方库的路径、我们的配置文件的路径以及我们的cuda指定。此外，我们还可以设置pytorch launch等来配置gpu之间的通信。由于我们现在只需要一个卡训练，所以不需要这个通讯。

![1753802007082](image/report/1753802007082.png)

我们可以看到已经正常训练了，然后也可以输出一轮完成之后的log、数据以及模型pth。

### 2.4 模型评估

我们的模型训练完毕之后会存放在

这时候我们查看模型启动的脚本文件sh，然后可以修改这个脚本，使之设置我们的环境变量并且调用test.py进行测试模型。

我们选取了对应的配置文件以及模型文件，

```bash

export PYTHONPATH=$PYTHONPATH:/home/lishengjie/study/jiahao/bupt_2507/isfusion

CONFIG=configs/isfusion/isfusion_0075voxel.py   

CHECKPOINT=/home/lishengjie/study/jiahao/bupt_2507/isfusion/work_dirs/isfusion_0075voxel/epoch8/latest.pth

CUDA_VISIBLE_DEVICES=3 \
python $(dirname "$0")/test.py $CONFIG $CHECKPOINT 

```

然后使用`bash tools/test.sh`来启动测试。这里的`--eval`是告诉测试模型使用评估模式，一共有`"--out", "--eval", "--format-only", "--show" or "--show-dir"`可以选择。由于我们的服务器没有可视化的gui，所以就不能使用show的命令。

```bash
bash tools/dist_test.sh --eval bbox --out /home/lishengjie/study/jiahao/bupt_2507/isfusion/output/pkls/test1.pkl 
```

最后，我们得到了评估结果，也就是每一轮训练之后得到的结果，我们会保存到pkl文件中。

![1753813055579](image/report/1753813055579.png)

### 2.5 结果可视化

同样这里也放在了test目录下，通过`--show_dir`以及`--show`我们可以得到可视化的结果。同时我们也可以在demo/目录中找到一个多模态的融合的demo。

首先我们要生成pkl文件并且保存。

```bash
bash tools/dist_test.sh --out /home/lishengjie/study/jiahao/bupt_2507/isfusion/output/pkls/all.pkl 
```

```bash
python demo/multi_modality_demo.py \
    data/nuscenes/samples/LIDAR_TOP/n008-2018-08-01-15-16-36-0400__LIDAR_TOP__1533151604547893.pcd.bin \
    data/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151604512404.jpg \
    data/nuscenes/nuscenes_infos_val.pkl \
    configs/isfusion/isfusion_0075voxel.py \
    work_dirs/isfusion_0075voxel/latest.pth \
    --out-dir ./demo_output
```

## 附录

### 1. 开源仓库

1. [IS-Fusion：](https://github.com/yinjunbo/IS-Fusion)`https://github.com/yinjunbo/IS-Fusion`

2. 

### 2. 参考论文

1. [IS-Fusion：](https://openaccess.thecvf.com/content/CVPR2024/papers/Yin_IS-Fusion_Instance-Scene_Collaborative_Fusion_for_Multimodal_3D_Object_Detection_CVPR_2024_paper.pdf)`https://openaccess.thecvf.com/content/CVPR2024/papers/Yin_IS-Fusion_Instance-Scene_Collaborative_Fusion_for_Multimodal_3D_Object_Detection_CVPR_2024_paper.pdf`